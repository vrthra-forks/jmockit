<!DOCTYPE html>
<!--
  ~ Copyright (c) 2006-2013 RogÃ©rio Liesenfeld
  ~ This file is subject to the terms of the MIT license (see LICENSE.txt).
  -->
<html>
<head>
   <title>The JMockit Testing Toolkit</title>
   <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
   <link rel="stylesheet" type="text/css" href="prettify.css"/>
   <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
   <script type="text/javascript" src="highlight.pack.js"></script>
   <script type="text/javascript">hljs.initHighlightingOnLoad()</script>
   <style type="text/css">#sections div { width: 25%; float: left; text-align: center; }</style>
</head>
<body>
<h2>
   The JMockit Testing Toolkit
   <a href="http://code.google.com/p/jmockit"><img align="right" src="tutorial/go-home.png" title="JMockit Home"></a>
</h2>
<p>
   JMockit is open-source software licensed under the
   <a href="http://www.opensource.org/licenses/mit-license.php">MIT License</a>.
   It is a collection of tools and APIs for use in <em>developer testing</em>, that is, tests written by developers
   using a testing framework such as <a href="http://junit.org">JUnit</a> or <a href="http://testng.org">TestNG</a>.
   The tools rely on the Java 5 SE instrumentation feature (the
   <a href="http://download.oracle.com/javase/6/docs/api/java/lang/instrument/package-summary.html"
      >java.lang.instrument</a> package), internally using the <a href="http://asm.objectweb.org">ASM</a> library to
   modify bytecode at runtime.
   Therefore, tests using JMockit must be run under a Java 5+ SE JVM.
</p>
<hr/>
<p>
   The following links provide a short overview of the different tools and APIs in the toolkit.
   The APIs (first three items) can be used for writing unit and integration tests.
   They enable the isolation of code under test from its dependencies, no matter what form they take or how they are
   obtained at runtime.
   The last item is a separate tool delivered in its own jar file, which can optionally be used while running test
   suites.
   The figure below is a clickable map with links to detailed documentation pages.
</p>
<div id="sections" style="margin-bottom: 32px">
   <div><a href="#expectations">JMockit Expectations</a></div>
   <div><a href="#verifications">JMockit Verifications</a></div>
   <div><a href="#mockups">JMockit Mockups</a></div>
   <div><a href="#coverage">JMockit Coverage</a></div>
</div>
<br/>
<div style="text-align: center;" title="Click on titles to open Tutorial/API">
   <map name="figure1">
      <area shape="rect" coords="15,12,165,33" href="tutorial/BehaviorBasedTesting.html">
      <area shape="rect" coords="45,92,179,125" href="javadoc/mockit/Expectations.html">
      <area shape="rect" coords="9,174,215,207" href="javadoc/mockit/NonStrictExpectations.html">
      <area shape="rect" coords="460,92,592,125" href="javadoc/mockit/Verifications.html">
      <area shape="rect" coords="234,174,428,207" href="javadoc/mockit/VerificationsInOrder.html">
      <area shape="rect" coords="447,174,605,207" href="javadoc/mockit/FullVerifications.html">
      <area shape="rect" coords="624,174,845,207" href="javadoc/mockit/FullVerificationsInOrder.html">
      <area shape="rect" coords="743,10,845,43" href="javadoc/mockit/Delegate.html">
      <area shape="rect" coords="726,59,845,92" href="javadoc/mockit/Invocation.html">
      <area shape="rect" coords="9,223,96,256" href="javadoc/mockit/Tested.html">
      <area shape="rect" coords="117,223,231,256" href="javadoc/mockit/Injectable.html">
      <area shape="rect" coords="252,223,346,256" href="javadoc/mockit/Mocked.html">
      <area shape="rect" coords="368,223,475,256" href="javadoc/mockit/NonStrict.html">
      <area shape="rect" coords="599,223,713,256" href="javadoc/mockit/Cascading.html">
      <area shape="rect" coords="733,223,845,256" href="javadoc/mockit/Capturing.html">

      <area shape="rect" coords="10,282,133,295" href="tutorial/StateBasedTesting.html">
      <area shape="rect" coords="9,305,138,338" href="javadoc/mockit/MockUp.html">
      <area shape="rect" coords="148,305,224,338" href="javadoc/mockit/Mock.html">

      <area shape="rect" coords="354,282,474,295" href="tutorial/ReflectionUtilities.html">
      <area shape="rect" coords="359,305,521,338" href="javadoc/mockit/Deencapsulation.html">
      
      <area shape="rect" coords="644,282,837,295" href="tutorial/UsingMocksAndStubs.html">
      <area shape="rect" coords="645,305,846,338" href="javadoc/mockit/UsingMocksAndStubs.html">
   </map>
   <img src="toolkit.png" usemap="#figure1">
</div>

<h3 id="motivation">Motivation</h3>
<p>
   This toolkit was created mainly as an attempt to overcome certain limitations found in "conventional"
   <a href="http://www.martinfowler.com/articles/mocksArentStubs.html">mocking</a> tools.
   Another goal was to provide simpler and more succinct APIs for writing developer tests.
   In addition, and differently from other testing tools which specifically target the use of mocks, JMockit also
   includes other tools designed to support the creation of large test suites.
</p>

<h4 id="conventionalTools">Conventional tools for mock objects</h4>
<p>
   The JMockit approach is an alternative to the conventional use of "mock objects" as provided by tools such as
   <a href="http://www.easymock.org">EasyMock</a> and <a href="http://www.jmock.org">jMock</a>.
</p>
<p>
   Both of those tools are based on
   <a href="http://download.oracle.com/javase/6/docs/api/java/lang/reflect/Proxy.html">java.lang.reflect.Proxy</a>,
   which requires an interface to be implemented.
   Additionally, they support the creation of mock objects for classes through
   <a href="http://cglib.sourceforge.net">CGLIB</a> subclass generation. Because of that, said classes cannot be
   <code>final</code> and only overridable instance methods can be mocked.
   <br/>
   Most importantly, however, when using these tools the dependencies of code under test (that is, the objects of other
   classes on which a given class under test depends) must be controlled by the tests, so that mock instances can be
   passed to the clients of those dependencies.
   Therefore, dependencies can't simply be instantiated with the <code>new</code> operator in a client class for which
   we want to write unit tests.
</p>
<p>
   Ultimately, the technical limitations of conventional mocking tools impose the following design restrictions on
   production code:
</p>
<ol>
   <li>
      Each class which may need to be mocked in a test must either implement a separate interface or not be
      <code>final</code>.
   </li>
   <li>
      The dependencies of each class to be tested must either be obtained through configurable instance creation methods
      (factories or a <em>Service Locator</em>), or be exposed for
      <a href="http://martinfowler.com/articles/injection.html">dependency injection</a>.
      Otherwise, unit tests won't be able to pass mock implementations of dependencies to the unit under test.
   </li>
   <li>
      Since only instance methods can be mocked, classes to be unit tested cannot call any static methods on their
      dependencies, nor instantiate them using any of the constructors.
   </li>
</ol>

<h4 id="productionCode">Design considerations for production code</h4>
<p>
   The problem with imposing restrictions on the design of production code is that there are good reasons for making
   classes and methods <code>final</code>, for directly obtaining instances of collaborators with the <code>new</code>
   operator, and for using APIs based on <code>static</code> methods.
   There is nothing inherently wrong with using these three Java language keywords, after all.
</p>
<p>
   In the first case, declaring classes or methods <code>final</code> makes it clear that they are not intended for
   extension by subclassing.
   For example, in a correct application of the <a href="http://en.wikipedia.org/wiki/Template_method_pattern">Template
   Method</a> design pattern, the "template" method itself (which implements an algorithm calling one or more
   overridable methods) must <em>not</em> be overridable.
   Also, from the point of view of API design, <code>final</code> classes are the only ones which can be safely evolved.
   For more on this, see <a href="http://www.gotw.ca/publications/mill18.htm">this article</a>.
   <br/>
   In practice, most classes and methods in an application or even in a reusable class library are not designed with
   extension by subclassing in mind, so it makes sense to declare them as <code>final</code>; probably that's why most
   other OO languages (C++, C#, etc.) make all methods non-overridable by default.
   <br/>
   Another benefit of making Java classes or methods <code>final</code> is that a good static analysis tool will be able
   to provide more useful feedback.
   The code inspections associated with the use of <code>final</code> in IntelliJ IDEA, for example, led me numerous
   times to simplify and even eliminate unused parts of the code base, when developing large business applications.
   (For the curious, some of the relevant inspections are: "Class structure: 'protected' member in 'final' class",
   "Declaration redundancy: redundant throws declaration", and "Initialization issues: overridable method call during
   object construction".)
   <br/>
   For an authoritative discussion on design for extension (which defends the judicious use of <code>final</code> for
   classes and methods), see "Item 17: Design and document for inheritance or else prohibit it" in the
   <a href="http://www.amazon.com/Effective-Java-2nd-Joshua-Bloch/dp/0321356683">Effective Java</a> book.
   Another book which strongly recommends the use of <code>final</code> for classes and methods is
   <a href="http://www.amazon.com/Practical-API-Design-Confessions-Framework/dp/1430209739">Practical API Design</a>
   (see "Make Everything Final", in chapter 5).
   Yet another relevant book is <a href="http://www.apibook.com/blog">API Design for C++</a>, which is also largely
   applicable to Java (see sections "2.3.2 Add Virtual Functions Judiciously" and "4.6.3 Using Inheritance").
   The site for this last book points to several interesting articles, including
   <a href="http://www.artima.com/weblogs/viewpost.jsp?thread=142428">Java API Design Guidelines</a>.
</p>
<p>
   In the second case, directly instantiating dependencies with <code>new</code> facilitates the use of stateful
   objects. In OO design, objects normally are not stateless. However, when dependencies are obtained or injected
   through external means (such as a "Service Locator" or a DI framework) there is a tendency to make them stateless,
   and with a single global instance. Such a practice leads to code that is more procedural and less object oriented.
   Of course, this applies to dependencies whose interfaces have only one implementation in production code, or when the
   selection of an implementation between many would not benefit from external configuration (for example, people 
   typically instantiate <code>List</code> implementations directly, even though they code to the abstract interface
   only); situations where one implementation <em>must</em> be selected through external configuration tend to be quite
   rare (at least in my experience).
   <br/>
   Related to this issue, I think, there is an widely observed misunderstanding of just what the phrase <cite>Program to
   an interface, not an implementation</cite> (from the famous "Design Patterns" book) actually means.
   Many seem to think that one should <em>create</em> a new separate Java interface (or abstract class) for any concrete
   class which don't yet implement one. In reality, it was only meant as a recommendation to avoid declaring variables,
   fields, parameters or return types as the implementation type <em>when an abstract type already exists</em> (for
   example, don't declare variables of type <code>ArrayList</code> if all you need is a <code>List</code>).
   The "Effective Java" book mentioned above also discusses this topic, in "Item 52: Refer to objects by their
   interfaces".
</p>
<p>
   In the third case, the use of classes containing only <code>static</code> methods is the best choice when none of the
   methods operate on any state (for example, the <code>Math</code> class), or the actual state is stored in a separate
   context object, such as the HTTP request context or the persistence context.
   For this last situation, consider an application that has a persistence subsystem which provides access to a
   relational database.
   One possibility is to inject instances of work unit objects (an Hibernate <code>Session</code>, or a JPA
   <code>EntityManager</code>) wherever they are needed, and use the persistence API directly.
   A better approach, in my experience, is to use a <em>static facade</em> which encapsulates all access from the
   application to the persistence subsystem.
   The benefits of this are many, not the least of which is that it actually provides <em>more</em> flexibility than the
   more direct, instance-based, approach.
   <br/>
   Another recommended usage of static methods is presented in "Item 1: Consider static factory methods instead of
   constructors", again in the "Effective Java" book.
</p>

<h4 id="testability">Testability</h4>
<p>
   The limitations found in conventional mocking tools has come to be associated with the idea of "untestable code".
   Often, we see the restrictions resulting from those limitations considered as inevitable, or even as something that
   could be beneficial.
   The JMockit toolkit, which breaks away from these limitations and restrictions, shows that in fact there is no such
   thing as truly untestable code. There is, of course, code that is harder to test because it is too complicated and
   convoluted, lacks cohesion, and so on and so forth.
</p>
<p>
   Therefore, by eliminating the technical limitations traditionally involved in the isolation of a unit from its
   dependencies, we get the benefit that no artificial design restrictions must be imposed on production code for the
   sake of unit testing.
   Additionally, it becomes possible to write unit tests for legacy code, without the need for any prior adaptation or
   refactoring.
   The ability to create automated regression tests for existing code before <em>refactoring</em> it is extremely
   valuable, as implied by the very <a href="http://martinfowler.com/bliki/DefinitionOfRefactoring.html">definition</a>
   of the term.
   In short, with a less restrictive mock testing tool the <em>testability</em> of production code becomes less of an
   issue, giving developers more freedom to use the language and more design choices when creating new code, while
   facilitating the initial creation of tests for legacy code.
</p>
<p>
   Another way of thinking about testability is to differentiate between <em>intrinsic</em> and <em>extrinsic</em>
   testability.
   In the first case, we can conclude that whatever makes the code more or less maintainable also makes it more or less
   easily testable, and vice-versa.
   For example, methods with higher <em>cyclomatic complexity</em> (basically, the number of different execution paths
   through the method) require more tests to be fully covered, while at the same time being more difficult to understand
   and change; in other words, intrinsic testability is not particularly useful as a separate measure for code quality,
   if it is nothing more than <em>maintainability</em>.
   In the second case, <em>extrinsic testability</em>, we are not really considering properties of the production code,
   but instead properties of the <em>tools</em> used to <em>test</em> that code.
   So, if a tool for testing code in isolation can provide an easy to use API that is able to deal with all possible
   isolation scenarios, then such extrinsic concerns for testability completely disappear.
</p>

<h4 id="mockingAPIs">Design considerations for mocking APIs</h4>
<p>
   The APIs for testing with mocks that are available in existing toolkits, even the more recent ones, have certain
   undesirable characteristics. Of course, this is partly a matter of personal taste, but some objective observations
   can certainly be made.
</p>
<p>
   In the following discussion, five different mocking toolkits (besides JMockit) are considered, all in their latest
   stable releases as of Feb 25, 2013: <strong>EasyMock 3.1</strong>, <strong>jMock 2.6.0</strong>,
   <strong>Mockito 1.9.5</strong>, <strong>PowerMock 1.5</strong>, and <strong>Unitils 3.3</strong>.
   The code snippets below come from the sample JMockit test suites that compare the JMockit approach with those other
   toolkits.
</p>
<ul>
   <li>
      Most mocking APIs rely on the imperative creation of mock instances through method calls, as opposed to a
      declarative approach where mocks are created implicitly from instance field declarations. Examples:
      <code>mock = createMock(Collaborator.class)</code> in EasyMock;
      <code>final Subscriber subscriber = context.mock(Subscriber.class)</code> in jMock;
      <code>myServiceMock = createMock(MyService.class)</code> in PowerMock (which also supports declarative mocks with
      the <code>@Mock</code> annotation on instance fields);
      <code>List&lt;String> mockedList = mock(List.class)</code> in Mockito (which, like PowerMock, has an optional
      <code>@Mock</code> annotation for instance fields).
      Unitils follows the declarative approach, but using a generic type for declaring mock fields, eg
      <code>Mock&lt;MessageService> mockMessageService</code>.
      The approach in JMockit Expectations is always declarative, with the use of coding conventions and annotations for
      instance fields as well as for <em>test method parameters</em> (the use of parameters to declare mocks is a
      feature unique to JMockit, and as examples will show, an extremely convenient one).
   </li>
   <li>
      When recording or verifying invocations on mocks, all other mocking APIs require mixing extra mocking API calls
      with regular calls to mocked methods. Some examples, with extra API calls in bold:
      <code><strong>expect</strong>(mock.voteForRemoval("Document"))</code> in EasyMock;
      <code><strong>oneOf</strong>(subscriber).receive(message)</code> in jMock;
      <code><strong>expect</strong>(myServiceMock.getAllPersons())</code> in PowerMock;
      <code><strong>when</strong>(mockedList.get(0))</code> and <code><strong>verify</strong>(mockedList).get(0)</code>
      in Mockito;
      <code>mockMessageService.<strong>assertInvoked()</strong>.sendMessage(alert1)</code> in Unitils.
      In the JMockit Expectations & Verifications API, you always call the mocked method directly, without any need to
      wrap or chain it with a mocking API specific call (there is only one exception to this, but it is rarely used).
   </li>
   <li>
      Just like with the other mocking APIs, there are calls in the JMockit Expectations API that need to be made for
      recording expected return values and thrown exceptions, for specifying invocation count constraints, and so on.
      Such calls are always kept separate from calls to mocked methods, though (actually, JMockit mostly uses
      <em>field assignments</em> instead of method calls). In other words, JMockit does not follow the
      <a href="http://martinfowler.com/dslwip/MethodChaining.html">method chaining</a> approach.
      Although it sometimes looks nice, the chaining of method calls tends to produce long and complex statements which
      can easily exceed the maximum line length, therefore requiring line wrapping at odd places.
      In addition, method chaining may require a different API when setting return values or thrown exceptions for
      <code>void</code> methods, since calls to such methods cannot be wrapped in an API call like
      <code>expect(&lt;call to mocked method>)</code>; both EasyMock and Mockito suffer from this problem, although that
      is not the case with jMock or Unitils.
   </li>
   <li>
      All mocking toolkits above, including JMockit, follow a <em>record-replay-verify</em> execution model for tests.
      These three execution <em>phases</em> divide the test in three consecutive steps:
      1) invocations to mocks are <em>recorded</em> with the desired return values or thrown exceptions, if any (each
      invocation is recorded as being <em>expected</em> or <em>allowed</em>);
      2) the code under test is exercised, when the recorded invocations to mocks can be <em>replayed</em>, resulting in
      the specified return values or thrown exceptions (here, invocations not recorded but allowed will simply result in
      default return values, while unexpected ones - if any - will result in assertion errors); and
      3) the invocations that did actually occur or not occur in the replay phase are <em>verified</em> against the
      invocations of interest (which were previously recorded as allowed ones, or not recorded at all but still
      allowed) in order to detect invocations that are either missing or occurring in excess.
      <br/>
      For any given test, both the first (<em>record</em>) and third (<em>verify</em>) phases can be empty, although at
      least one of them will always be specified. The <em>replay</em> phase, obviously, always exists.
      <br/>
      The issue I am getting at here is that most other mocking toolkits require explicit coding for transitioning a
      test between phases, while none of them helps the developer to quickly and easily visualize the separate phases of
      the test.
      In EasyMock, you need to call <code>replay(mock)</code> to switch from <em>record</em> to <em>replay</em>, and
      then later call <code>verify(mock)</code> to switch from <em>replay</em> to <em>verify</em> (alternatively, there
      is a base test class that provides <code>replayAll()</code> and <code>verifyAll()</code> helper methods).
      PowerMock, when using the EasyMock API, also relies on calls to <code>replayAll()</code> and
      <code>verifyAll()</code>.
      Mockito and Unitils are better on this point, but at the cost of requiring explicit API calls for every invocation
      to be recorded or verified.
      jMock is similar to the JMockit Expectations syntax, where recorded invocations are those written inside an
      <em>Expectations</em> code block (an anonymous inner class containing one instance initialization block but no
      method implementations).
      However, jMock has nothing like the <em>Verifications</em> block available in JMockit, and each Expectations
      instance must be passed as argument in a call to the <code>Mockery#checking</code> method, which transitions the
      test from <em>record</em> to <em>replay</em>.
      <br/>
      The JMockit Expectations & Verifications API relies on the instance initialization of anonymous inner classes to
      demarcate the <em>record</em> and <em>verify</em> phases.
      This does require extra indentation levels and curly braces, but the advantages are big: the explicit
      expectation/verification blocks avoid repetition of method calls like "expect(...)" and "verify(...)", while
      adding structure and readability to the test by clearly and cleanly separating code which belongs to each of the
      three phases.
   </li>
</ul>
<p>
   There are other less dramatic differences in style between JMockit and other mocking APIs, but the above items should
   be enough to show that many possibilities exist in the realm of mocking API design.
</p>

<h3 id="alternativeTools">Alternative mocking tools</h3>
<p>
   There are now other mocking tools for Java which also overcome the limitations of the conventional ones, between them
   <a href="http://code.google.com/p/powermock">PowerMock</a>,
   <a href="http://java.net/projects/jeasytest">jEasyTest</a>, and
   <a href="http://mockinject.codehaus.org">MockInject</a>.
   The one that comes closest to the feature set of JMockit is PowerMock, so I will briefly evaluate it here (besides,
   the other two are more limited and don't seem to be actively developed anymore).
</p>
<dl id="PowerMock"><dd><strong>JMockit</strong> vs <strong>PowerMock</strong></dd>
<ul>
<li>
   First of all, PowerMock does not provide a complete API for mocking, but instead works as an extension to another
   tool, which currently can be EasyMock or Mockito.
   This is obviously an advantage for existing users of those tools.
   <br/>
   JMockit, on the other hand, provides entirely new APIs, although its main API (Expectations) is similar to both
   EasyMock and jMock.
   While this creates a longer learning curve, it also allows JMockit to provide a simpler, more consistent, and easier
   to use API.
</li>
<li>
   Compared to the JMockit Expectations API, the PowerMock API is more "low-level", forcing users to figure out and
   specify which classes need to be prepared for testing (with the <code>@PrepareForTest({ClassA.class, ...})</code>
   annotation) and requiring specific API calls to deal with various kinds of language constructs that may be present in
   the production code: static methods (<code>mockStatic(ClassA.class)</code>),
   constructors (<code>suppress(constructor(ClassXyz.class))</code>),
   constructor invocations (<code>expectNew(AClass.class)</code>),
   partial mocks (<code>createPartialMock(ClassX.class, "methodToMock")</code>), etc.
   <br/>
   With JMockit Expectations, all kinds of methods and constructors are mocked in a purely declarative way, with partial
   mocking specified through regular expressions in the <code>@Mocked</code> annotation or by simply "un-mocking" the
   members with no recorded expectations; that is, the developer simply declares some shared "mock fields" for the test
   class, or some "local mock fields" and/or "mock parameters" for individual test methods (and in this last case the
   <code>@Mocked</code> annotation often won't be needed).
</li>
<li>
   Some capabilities available in JMockit, such as support for mocking <code>equals</code> and <code>hashCode</code>,
   overridden methods, and others, are currently not supported in PowerMock.
   Also, there is no equivalent to JMockit's ability to capture instances and mock implementations of specified base
   types as the test executes, without the test code itself having any knowledge of the actual implementation classes.
</li>
<li>
   PowerMock uses custom class loaders (usually one per test class) in order to generate modified versions of the mocked
   classes.
   Such heavy use of custom class loaders can lead to conflicts with third-party libraries, hence the need to sometimes
   use the <code>@PowerMockIgnore("package.to.be.ignored")</code> annotation on test classes.
   <br/>
   The mechanism used by JMockit (runtime instrumentation through a "Java agent") is simpler and safer, although it does
   require passing a "-javaagent" parameter to the JVM when developing on JDK 1.5; on JDK 1.6+ (which can always be used
   for development, even if deploying on an older version) there is no such requirement, since JMockit can transparently
   load the Java agent on demand by using the Attach API.
</li>
</ul></dl>

<p>
   Another recent mocking tool is <a href="http://code.google.com/p/mockito">Mockito</a>.
   Although it does not attempt to overcome the limitations of older tools (jMock, EasyMock), it does introduce a new
   style of behavior testing with mocks.
   JMockit also supports this alternative style, through the Verifications API.
</p>
<dl id="Mockito"><dd><strong>JMockit</strong> vs <strong>Mockito</strong></dd>
<ul>
<li>
   Mockito relies on explicit calls to its API in order to separate code between the <em>record</em>
   (<code>when(...)</code>) and <em>verify</em> (<code>verify(...)</code>) phases.
   This means that any invocation to a mock object in test code will also require a call to the mocking API.
   Additionally, this will often lead to repetitive <code>when(...)</code> and <code>verify(mock)...</code> calls.
   <br/>
   With JMockit, no similar calls exist. Sure, we have the <code>new NonStrictExpectations()</code> and
   <code>new Verifications()</code> constructor calls, but they occur only once per test (typically), and are completely
   separate from the invocations to mocked methods and constructors.
</li>
<li>
   The Mockito API contains several inconsistencies in the syntax used for invocations to mocked methods.
   In the <em>record</em> phase, we have calls like <code>when<strong>(mock.mockedMethod(args))</strong>...</code> while
   in the <em>verify</em> phase this same call will be written as
   <code>verify<strong>(mock)</strong>.mockedMethod(args)</code>.
   Notice that in the first case the invocation to <code>mockedMethod</code> is made directly on the
   <code>mock</code> object, while in the second case it is made on the object returned by <code>verify(mock)</code>.
   <br/>
   JMockit has no such inconsistencies because invocations to mocked methods are always made directly on the mocked
   instances themselves.
   (With one exception only: to match invocations on the same mocked instance, an <code>onInstance(mock)</code> call is
   used, resulting in code like <code>onInstance(mock).mockedMethod(args)</code>; most tests won't need to use this,
   though.)
   <br/>
   Just like other mocking tools which rely on method chaining/wrapping, Mockito also runs into inconsistent syntax when
   stubbing <code>void</code> methods.
   For example, you write <code>when(mockedList.get(1)).thenThrow(new RuntimeException());</code> for a
   non-<code>void</code> method, and <code>doThrow(new RuntimeException()).when(mockedList).clear();</code> for a
   <code>void</code> one.
   With JMockit, it's always the same syntax:
   <code>mockedList.clear(); <em>result</em> = new RuntimeException();</code>.
   <br/>
   Yet another inconsistency occurs in the use of Mockito <em>spies</em>: "mocks" that allow the real methods to be
   executed on the spied instance.
   For example, if <code>spy</code> refers to an empty <code>List</code>, then instead of writing
   <code>when(spy.get(0)).thenReturn("foo")</code> you will need to write <code>doReturn("foo").when(spy).get(0)</code>.
   With JMockit, the <em>dynamic mocking</em> feature provides similar functionality to spies, but without this issue
   since real methods only get executed during the <em>replay</em> phase.
</li>
<li>
   In EasyMock and jMock, the first mocking APIs for Java, the focus was entirely on the recording of <em>expected
   invocations</em> of mocked methods, for mock objects that (by default) do not allow unexpected invocations.
   Those APIs also provide the recording of <em>allowed invocations</em> for mock objects that do allow unexpected
   invocations, but this was treated as a second-class feature.
   Additionally, with these tools there is no way to explicitly verify invocations to mocks after the code under test is
   exercised. All such verifications are performed implicitly and automatically.
   <br/>
   In Mockito (and also in Unitils Mock), the opposite viewpoint is taken.
   All invocations to mock objects that may happen during the test, whether recorded or not, are <em>allowed</em>, never
   <em>expected</em>.
   Verification is performed explicitly after the code under test is exercised, never automatically.
   <br/>
   Both approaches are too extreme, and consequently less than optimal.
   JMockit Expectations & Verifications is the only API that allows the developer to seamlessly choose the best
   combination of <em>strict</em> (expected by default) and <em>non-strict</em> (allowed by default) mock invocations
   for each test.
   <br/>
   To be more clear, the Mockito API has the following shortcoming. If you need to verify that an invocation to a
   non-<code>void</code> mocked method happened during the test, but the test requires a return value from that method
   that is different from the default for the return type, then the Mockito test will have duplicate code: a
   <code>when(mock.someMethod()).thenReturn(xyz)</code> call in the <em>record</em> phase, and a
   <code>verify(mock).someMethod()</code> in the <em>verify</em> phase.
   With JMockit, a <em>strict</em> expectation can always be recorded, which won't have to be explicitly verified.
   Alternatively, an invocation count constraint (<code><em>times</em> = 1</code>) can be specified for any recorded
   <em>non-strict</em> expectation (with Mockito such constraints can only be specified in a
   <code>verify(mock, <em>constraint</em>)</code> call).
</li>
<li>
   Mockito has poor syntax for verifications <em>in order</em>, and for <em>full</em> verifications (that is, checking
   that all invocations to mock objects are explicitly verified).
   In the first case, an extra object needs to be created, and calls to <code>verify</code> made on it:
   <code>InOrder inOrder = inOrder(mock1, mock2, ...)</code>.
   In the second case, calls like <code>verifyNoMoreInteractions(mock)</code> or
   <code>verifyZeroInteractions(mock1, mock2)</code> need to be made.
   <br/>
   With JMockit, you simply write <code>new VerificationsInOrder()</code> or <code>new FullVerifications()</code>
   instead of <code>new Verifications()</code> (or <code>new FullVerificationsInOrder()</code> to combine both
   requirements). No need to specify which mock objects are involved. No extra mocking API calls.
   And as a bonus, by calling <code>unverifiedInvocations()</code> inside an ordered verification block, you can
   perform order-related verifications that are simply impossible in Mockito.
</li>
</ul></dl>

<p>
   Finally, the JMockit Testing Toolkit has a wider scope and more ambitious goals than other mocking toolkits, in order
   to provide a complete and sophisticated developer testing solution.
   A good API for mocking, even without artificial limitations, is not enough for productive creation of tests.
   An IDE-agnostic, easy to use, and well integrated Code Coverage tool is also essential, and that's what JMockit
   Coverage aims to provide.
</p>

<h3 id="expectations">JMockit Expectations</h3>
<p>
   The <em>JMockit Expectations</em> mocking API provides a <em>record-replay</em> model for writing
   <em>behavior-based</em> tests.
   In this model, a test begins by setting one or more <em>expectations</em> on the invocations made from code under
   test to its collaborators (dependencies).
   The classes and instances for such dependencies are established through the declaration of one or more <em>mocked
   types</em> inside the test class/method.
   Such mocked types can be declared through instance fields of the test class or of an <code>Expectations</code>
   anonymous subclass inside a test method, and also through parameters of test methods (even though JUnit and TestNG
   don't allow regular test methods to have parameters).
   After expectations are defined in this <em>recording phase</em>, the test transitions to the <em>replay phase</em>,
   when the code under test is exercised. The invocations that actually occur on the mocked collaborators are handled
   according to the mocked type declarations and the corresponding expectations recorded on them (if any).
   At the end of the replay phase, those expectations for which one or more invocations were expected are automatically
   verified so that missing invocations can be detected.
</p>
<p>
   Expectations can be specified on any kind of method invocation (on interfaces, abstract classes, concrete final or
   non-final classes, and on static methods), as well as on class instantiation through any constructors.
   Private methods/constructors can also have expectations defined.
</p>
<p>
   This API provides several ways to specify <em>argument matching</em> constraints on invocations, including the use of
   custom <a href="http://code.google.com/p/hamcrest">Hamcrest matchers</a>.
   There are special methods and <em>fields</em> through which <em>values to return</em> or <em>exceptions to throw</em>
   can be specified for all kinds of mock invocations.
   Other API fields can be used to specify lower and/or upper limits on the <em>number</em> of expected and/or allowed
   invocations, respectively, for a given expectation.
</p>
<pre><code>public class JMockitExpectationsExampleTest
{
   // Common mock fields can be declared here, and must be annotated with @Mocked.
   
   @Test
   public void testDoOperationAbc()
   {
      new Expectations() {
         // This is a local mock field; it can optionally be annotated with @Mocked.
         DependencyXyz mock; // a mocked instance is automatically created and assigned

         {
            new DependencyXyz(); // records an expectation on a constructor invocation
            mock.doSomething("test"); <em>result</em> = 123;
   
            // The expectations above are <strong>strict</strong>, causing the whole dependency to be strictly verified.
            // Therefore, invocations not recorded here will be considered <strong>unexpected</strong>, causing the test
            // to fail if they occur while exercising the code under test.
            // <strong>Non-strict</strong> expectations are also supported.
         }
      };

      // In ServiceAbc#doOperationAbc(String s): "new DependencyXyz().doSomething(s);"
      Object result = new ServiceAbc().doOperationAbc("test");

      assertNotNull(result);

      // That all expected invocations were actually executed in the replay phase is automatically
      // verified at this point, through transparent integration with the JUnit/TestNG test runner.
   }
}
</code></pre>
<div class="links">
   <a href="tutorial/AnExample.html#expectations">Tutorial sample</a>
   <a href="tutorial/BehaviorBasedTesting.html">Tutorial chapter</a>
   <a href="javadoc/mockit/Expectations.html">API documentation</a>
   <br/>
   <a href="http://code.google.com/p/jmockit/source/browse/trunk/samples/easymock">Sample for comparison with
      EasyMock</a>
   <a href="http://code.google.com/p/jmockit/source/browse/trunk/samples/jmock">Sample for comparison with jMock</a>
</div>

<h3 id="verifications">JMockit Verifications</h3>
<p>
   This API is a natural extension of the Expectations API, where the <em>record-replay</em> model gets an extra phase
   and becomes the <em>record-replay-verify</em> model for behavior-based testing.
</p>
<p>
   By default, all expectations recorded inside an <code class="type">Expectations</code> block are <em>strict</em>,
   which means that for each expectation recorded a matching invocation must be executed during the replay phase of the
   test, and in the same order as recorded.
   Additionally, if during the replay phase an <em>unexpected</em> invocation (ie, one not matching a recorded strict
   expectation) to one of the mocked types/instances is detected, then an assertion error will be thrown.
   (The number of invocations for each strict expectation is flexible, though, since lower/upper limits can be specified
   individually.)
</p>
<p>
   Inside an expectation block, however, individual expectations can be marked as being <em>non-strict</em>.
   When all invocations to a given mocked type should be non-strict, the
   <strong><code class="annotation">@NonStrict</code></strong> annotation can be used.
   Finally, if all expectations to be recorded (on one or more mocked types/instances in scope for the same test) should
   be non-strict, the <strong><code class="type">NonStrictExpectations</code></strong> class can be used instead of
   <code class="type">Expectations</code>.
</p>
<p>
   A non-strict expectation, then, is one that by default can be invoked any number of times (including zero) during the
   replay phase, and in arbitrary order.
   Similarly, any unexpected invocation to a non-strict mocked type/instance does <strong>not</strong> cause an error to
   be thrown.
</p>
<p>
   In this model, invocations to non-void methods that occur in the replay phase and that need a return value different
   from the default one must still be recorded, as they normally would need if the expectations were strict.
   All other invocations, on the other hand, can be <em>verified</em> to have occurred (or not) <em>after</em> the
   replay phase, in the same place regular JUnit assertions would be.
   This is done in the <em>verification</em> phase of the test, which is demarcated by a
   <strong><code class="type">Verifications</code></strong> block (or a variant, such as
   <code class="type">VerificationsInOrder</code>).
   It's also possible to have <em>no</em> expectations recorded for a given test, in which case there will be no
   expectation block in the test method, but only the verification block.
</p>
<pre><code>public class JMockitVerificationsExampleTest
{
   @Test // notice the "mock parameter", whose argument value will be created automatically
   public void testDoAnotherOperation(final AnotherDependency anotherMock)
   {
      new NonStrictExpectations() {
         DependencyXyz mock; // mock instance created and assigned automatically

         {
            mock.doSomething("test"); <em>result</em> = 123; <em>times</em> = 1;
         }
      };

      // In ServiceAbc#doAnotherOperationAbc(String s): "new DependencyXyz().doSomething(s);"
      // and "new AnotherDependency().complexOperation(1, obj);".
      new ServiceAbc().doAnotherOperation("test");

      new Verifications() {{
         anotherMock.complexOperation(<em>anyInt</em>, null);
      }};
   }
}
</code></pre>
<div class="links">
   <a href="tutorial/AnExample.html#verifications">Tutorial sample</a>
   <a href="tutorial/BehaviorBasedTesting.html#verification">Tutorial chapter</a>
   <a href="javadoc/mockit/Verifications.html">API documentation</a>
   <br/>
   <a href="http://code.google.com/p/jmockit/source/browse/trunk/samples/mockito">Sample for comparison with Mockito</a>
   <a href="http://code.google.com/p/jmockit/source/browse/trunk/samples/unitils">Sample for comparison with Unitils
      Mock</a>
</div>

<h3 id="mockups">JMockit Mockups</h3>
<p>
   This is a different kind of mocking API, which can be seen as complementary to the Expectations & Verifications API.
   Instead of specifying expectations on the invocations made from code under test to its collaborators,
   <em>mock classes</em> are defined and applied for the scope of a single test method or a whole test class.
   Inside these mock classes, <em>mock methods</em> (indicated as such with the
   <strong><code class="annotation">@Mock</code></strong> annotation) are directly implemented with code that will be
   executed instead of the original code for the corresponding <em>mocked method/constructor</em>.
   In addition, constraints on the number of expected invocations for each mock method can be specified, and mock
   methods can be made to be <em>re-entrant</em>.
</p>
<pre><code>public class JMockitMockupsExampleTest
{
   @Test
   public void testDoOperationAbc()
   {
      // A "mock-up" class, defined and applied at the same time:
      new MockUp&lt;DependencyXyz>() {
         @Mock(invocations = 1)
         int doSomething(String value)
         {
            assertEquals("test", value);
            return 123;
         }
      };

      // In ServiceAbc#doOperationAbc(String s): "new DependencyXyz().doSomething(s);"
      Object result = new ServiceAbc().doOperationAbc("test");

      assertNotNull(result);
   }
}
</code></pre>
<div class="links">
   <a href="tutorial/AnExample.html#mockups">Tutorial sample</a>
   <a href="tutorial/StateBasedTesting.html">Tutorial chapter</a>
   <a href="javadoc/mockit/Mock.html">API documentation</a>
</div>

<h3 id="coverage">JMockit Coverage</h3>
<p>
   Existing OpenSource code coverage tools, such as <a href="http://cobertura.sourceforge.net">Cobertura</a> and
   <a href="http://emma.sourceforge.net">EMMA</a> are also less than ideal.
   Specifically, JMockit Coverage provides the following benefits.
</p>
<ol>
   <li>
      Bytecode modification performed only at runtime, therefore avoiding the creation of undesirable files.
      No extra source or class files are created, and no coverage data file is generated unless explicitly requested.
      The only files created or modified are those that the user really wants as the desired output.
   </li>
   <li>
      Running tests with JMockit Coverage does not require the use of any particular command line script, Ant task, or
      IDE plug-in.
      The tool applies the idea of convention over configuration, trying to make it as easy as possible for the
      developer to run tests with code coverage: by simply adding one jar file to the classpath, and optionally
      specifying certain initialization parameters to the JVM (system properties settable with "<code>-D</code>" or its
      Ant/Maven equivalent).
   </li>
   <li>
      No need to specify which classes should be considered for coverage (it still can be specified if needed, though).
      All code under test will automatically be analyzed for coverage.
      Specifically, the tool will gather coverage data for all production code executed by the test suite, excluding
      classes defined inside jar files (on the assumption that such code belongs to libraries used by the code under
      test).
   </li>
   <li>
      An HTML report and/or a serialized output file can be easily generated at the end of each test run.
      The first requires no extra configuration as it is the default form of coverage output; the second only requires
      the trivial effort of setting the "<code>jmockit-coverage-output</code>" system property.
      For the generation of the HTML report, Java source files are automatically searched in all "src" directories
      under the working directory.
   </li>
   <li>
      In code coverage reports, each line of production code can be accurately linked to the lines
      in test code which caused their execution, for each individual execution of each line of code.
      (This feature is inactive by default, however, because of the higher runtime cost and the larger resulting HTML
      report.)
   </li>
   <li>
      Besides a <em>line coverage</em> metric, a <em>path coverage</em> metric is also made available.
      Both metrics are calculated and shown at the source file, package, and global levels.
      For path coverage, each possible path through a method or constructor can be interactively displayed in the HTML
      report.
   </li>
   <li>
      <em>Intra-line</em> coverage is provided, with the appropriate red/green coloring in the HTML report for each
      conditionally executed line segment.
   </li>
</ol>
<hr/>
<div class="links">
  <a href="tutorial/CodeCoverage.html">Running tests with JMockit Coverage</a>
  <a href="http://jmockit.googlecode.com/svn/trunk/www/coverage-sample/index.html">Sample coverage report</a>
</div>

</body>
</html>
